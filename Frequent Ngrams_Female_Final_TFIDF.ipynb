{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2918261\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\2918261\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\2918261\\\\Courses\\\\Oslo Summer School\\\\notebooks\\\\Interviews\\\\Text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C://Users//2918261//Courses//Oslo Summer School//notebooks//Interviews//Text')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews = pd.read_json('C://Users//2918261//Dropbox//Corner Office Interviews//ComparisonCorpusText//Json Tables for ML//interviews_full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews = interviews[interviews['Female']== 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694653"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews.text.str.split().map(len).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = interviews.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There\\xa0has never been a more tumultuous time in our industry. \\xa0\\n'\n",
      " 'The\\xa0lines between hackers hobbyists and nation state attackers are '\n",
      " 'continuously blurred with security leaders having\\xa0to scramble to defend '\n",
      " 'against an ever-evolving slate of attacks. \\n'\n",
      " 'A CISO today has no idea if valuable data is being taken to make a national '\n",
      " 'statement by someone with a vested interest or purely for the market value. '\n",
      " 'Because of this \\n'\n",
      " 'one of the more interesting trends we have been seeing is the focus on the '\n",
      " 'vulnerable insider or employee  as a\\xa0potential\\xa0root because\\xa0of any '\n",
      " 'of the\\xa0three scenarios\\xa0above.\\n'\n",
      " 'As we move forward however and\\xa0AI becomes a critical tool for both hacker '\n",
      " 'as well as defender\\xa0the why will matter increasingly less. \\n'\n",
      " 'The\\xa0ability of each side to fulfil their mission will rely on '\n",
      " 'visibility\\xa0and the agility. The CISO needs to focus on lightweight \\n'\n",
      " 'high-fidelity\\xa0data collection\\xa0to be able to identify and respond to '\n",
      " 'new risks in real time  coupled with transparency and rapid learning. \\n'\n",
      " 'Analytics\\xa0engines running in batch\\xa0mode to cope with massive\\xa0amount '\n",
      " 'of heavy unfiltered data\\xa0will not scale to meet the challenge.\\n'\n",
      " 'Australia continues to have a vibrant security community  but as we see with '\n",
      " 'many other Australian startups \\n'\n",
      " 'it became necessary for Dtex to move to Silicon Valley in order to connect '\n",
      " 'with the talent and funding pool needed to grow and scale a business.\\n'\n",
      " 'We were thrilled this year to be able to re-enter the Australian market '\n",
      " 'which is one of the more mature and forward-thinking at present. \\xa0\\n'\n",
      " 'It remains a high-growth business opportunity and area of focus for Dtex and '\n",
      " 'we will continue to invest.\\n'\n",
      " 'Unfortunately you cannot look at any of these items as static. The only '\n",
      " 'constant in the\\xa0world of security is change and increasingly \\n'\n",
      " 'our success both as vendors and defenders will be dependent on our ability '\n",
      " 'to adapt - and how we respond to both the market and risk. \\n'\n",
      " 'We have found in working with Australian partners customers and '\n",
      " 'technologists that there is a depth in both security understanding and '\n",
      " 'ability. \\xa0\\n'\n",
      " 'And some of the most forward looking CISOs that we have had the honor to '\n",
      " 'work with have been within Australia.\\n'\n",
      " 'Cyber is a world of change. \\xa0If the last decade has taught us nothing '\n",
      " 'else it is that whatever you fear today is not going to be what you fear\\xa0'\n",
      " 'tomorrow. \\n'\n",
      " 'The risk landscape is continuously evolving. \\xa0If I look at extremely '\n",
      " 'mature industries - like financial services or the public sector \\n'\n",
      " 'they have become much more aggressive in evaluating and embracing new '\n",
      " 'technologies. Evaluation and procurement cycles that were\\xa0previously '\n",
      " 'measured in months \\n'\n",
      " 'in some areas have been cut in half.\\n'\n",
      " 'This adaptability and capacity to absorb new technology is what is going to '\n",
      " 'keep the enterprise immune system strong. \\n'\n",
      " 'The inverse is the enterprises ability to cycle out old or underperforming '\n",
      " 'technologies. This is an area where I believe there is more work to do.\\n'\n",
      " 'Ideologically I am a\\xa0believer\\xa0that strength comes from transparency '\n",
      " 'and collaboration. Practically speaking however this is much harder. \\xa0\\n'\n",
      " 'I think the concept of openly sharing data is entirely achievable - but a '\n",
      " 'centralized \\n'\n",
      " 'global cyber immune system is much less practical as every country has its '\n",
      " 'own definition of risk. \\xa0\\n'\n",
      " 'We can all agree on the identity of an individual - but what the US deems to '\n",
      " 'be a risk may not be consistent with the view in China. \\n'\n",
      " 'I do believe we should invest in global information-sharing capabilities '\n",
      " 'which can still go a long way to providing a platform for creating\\xa0'\n",
      " 'compatible cyber systems.\\n'\n",
      " 'I think its important to recognize that David Irvine ran ASIO and ASIS up '\n",
      " 'until 2014  and since that time the Australian cybersecurity market has '\n",
      " 'matured significantly. \\n'\n",
      " 'However I would tend to agree with his suggestion that a single '\n",
      " 'Commonwealth-led cooperative agency could help to break down silos and '\n",
      " 'present a more coordinated approach \\n'\n",
      " 'to combating cybercrime.\\n'\n",
      " 'Its also important to note that the Australian Government has recently taken '\n",
      " 'steps towards a more \\n'\n",
      " 'centralized approach by consolidating its cyber teams including those of the '\n",
      " 'DTA Digital Transformation Agency and the ASD Australian Signals '\n",
      " 'Directorate. \\n'\n",
      " 'This demonstrates a major shift towards the consolidated approach '\n",
      " 'recommended by David.\\n'\n",
      " ' The first leadership role I can remember was in elementary school. They had '\n",
      " 'a choir but when I got to third grade they did not have a teacher for the '\n",
      " 'choir anymore. So I became the instructor after convincing the music '\n",
      " 'department to let me and a friend who played the piano do it. So we told the '\n",
      " 'first- and second-graders that wed create a choir. It seemed wrong that '\n",
      " 'there was not going to be a choir because I had had one when I was in the '\n",
      " 'first and second grade.\\n'\n",
      " ' My mom did mostly office work and my father had a variety of different '\n",
      " 'jobs. We were not an affluent family by any stretch of the imagination. So '\n",
      " 'my mother would often say If you want something you are going to have to '\n",
      " 'figure out how to go get it because nobodys going to bring it to you.\\n'\n",
      " 'My grandfather was also a big influence. He moved his whole family from '\n",
      " 'Holland to Canada after World War II and came in as an orchard worker in '\n",
      " 'British Columbia. He lived in a little shack in an orchard and worked for '\n",
      " 'one of the local families. Then he built a bigger house for him and his '\n",
      " 'family and then he built somebody else a house and he eventually started his '\n",
      " 'own construction company. So I spent weekends in his workshop. If I wanted '\n",
      " 'something he would show me how to build it  he would not build it for me. '\n",
      " 'Anything you want you just need to make it he would say.\\n'\n",
      " ' I went to the College of Geographic Sciences in Nova Scotia. It was very '\n",
      " 'small and it was very intense  a really brutal course. On the first day they '\n",
      " 'said Look around you. Fifty percent of you will be gone by Christmas. I '\n",
      " 'remember feeling that everyone was looking at me because I was only one of '\n",
      " 'three women in the room. They were thinking that is the first one who is '\n",
      " 'going.\\n'\n",
      " 'I was very intimidated at first because I had no technical background. I had '\n",
      " 'barely touched a computer. Even though everybody assumed I would be the '\n",
      " 'first to go I made sure I was the last to go. By Christmas I was actually '\n",
      " 'head of the class.\\n'\n",
      " ' there is the work side versus the personal side of managing a team. I am '\n",
      " 'very driven and I am very focused on what we need to get done. So early on I '\n",
      " 'had to kind of remind myself to hit the pause button check in with folks and '\n",
      " 'make sure they are with you as you are going forward.\\n'\n",
      " ' One lesson from both Apple and Motorola was the importance of getting into '\n",
      " 'the details  just the attention to every pixel every detail every word. When '\n",
      " 'Sanjay Jha came in to rebuild Motorola he really brought us back to grinding '\n",
      " 'through every detail. How are we designing products How are we looking at '\n",
      " 'the market How are we talking to customers\\n'\n",
      " 'So it was a 20000-foot view but he could also take it down to a '\n",
      " 'centimeter-off-the-ground view and in a way that did not slow us down. '\n",
      " 'Because that is the big risk  if you try to get into that much detail how do '\n",
      " 'you not slow your organization down The point is that its not about '\n",
      " 'micromanaging its about asking the right questions.\\n'\n",
      " ' From a day-to-day perspective its not all that different from when I was a '\n",
      " 'general manager running a big P.L. But I have noticed that people read a lot '\n",
      " 'more meaning into things that you did not necessarily intend to have '\n",
      " 'meaning. People will make up stories in the white space.\\n'\n",
      " 'We had one recent example. Were a Silicon Valley company so we have a very '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'full kitchen. I hired a new head of business operations and she decided we '\n",
      " 'were going to switch out the vendors. There was a week when the supply went '\n",
      " 'very low because the next vendor was coming in a couple of weeks later to '\n",
      " 'kind of set up. Because we had not said anything about it and the food was '\n",
      " 'starting to run low people started saying there is layoffs coming bad things '\n",
      " 'are going to happen.\\n'\n",
      " 'I actually had to say in an all-hands meeting Guys its just the nuts in the '\n",
      " 'kitchen. that is it. But people look for symbols and they look for meaning '\n",
      " 'where maybe there is not any. So now were overcommunicating. You have to '\n",
      " 'talk about the little stuff as well as the big stuff just to make sure folks '\n",
      " 'are not running away with ideas.\\n'\n",
      " ' I go on two things. One is intuition. I am relatively good at reading '\n",
      " 'people reading their authenticity and whether what they are saying is '\n",
      " 'connected to underlying facts.\\n'\n",
      " 'That shows up early in the conversation. Are they looking you in the eye Are '\n",
      " 'they comfortable with themselves\\n'\n",
      " 'Then I want very specifically to know what they have done. If they say We '\n",
      " 'did this amazing thing as a team my next questions are usually going to be '\n",
      " 'So what part of that did you do How did you approach that How did you know '\n",
      " 'that was the right answer I also try to understand whether people will lean '\n",
      " 'into tough problems or lean away from them.\\n'\n",
      " ' I will say Give me an example of a time you were responsible for something '\n",
      " 'and it was not working. What did you do I want people who are going to lean '\n",
      " 'in and take ownership. You want people with the right level of inspiration '\n",
      " 'and passion and commitment. You want people who take ownership of the '\n",
      " 'outcome. they are going to get it done.\\n']\n"
     ]
    }
   ],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing and cleaning up with gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "interviews['data_words'] = list(sent_to_words(data)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103    [there, has, never, been, more, tumultuous, time, in, our, industry, the, lines, between, hackers, hobbyists, and, n...\n",
      "Name: data_words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(interviews['data_words'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename',\n",
       " 'text',\n",
       " 'Name ',\n",
       " 'Gender',\n",
       " 'Organization',\n",
       " 'Current Age',\n",
       " 'NativeSpeaker',\n",
       " 'Traded',\n",
       " 'MBA',\n",
       " 'Sector_Grouped',\n",
       " 'Female',\n",
       " 'Sector_Grouped_Dic',\n",
       " 'Femininity',\n",
       " 'Masculinity',\n",
       " 'data_words']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(interviews.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews['data_words_stemmed'] = interviews['data_words'].apply(lambda x: word_stemmer(x)).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews['data_words_stemmed_lemmatized']=interviews['data_words_stemmed'].apply(wnl.lemmatize).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650501"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.split().map(len).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting tree tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('women').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('women').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('woman').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('young').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('guy').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('love').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1859"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('compani').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2414"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('men').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('feel').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('use').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('probabl').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('custom').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('mother').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('softwar').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('better').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('famili').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('someon').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('role').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('opportun').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('children').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviews['data_words_stemmed_lemmatized'].str.count('word').values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecting tokens with Tfidf vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range = (1, 1),\n",
    "                             min_df = 0.2,\n",
    "                             max_df = 0.9, # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # '[a-zA-Z0-9]{3,}' num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(interviews['data_words_stemmed_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(data_vectorized.toarray(),\n",
    "         columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204 entries, 0 to 203\n",
      "Columns: 652 entries, abil to younger\n",
      "dtypes: float64(652)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "words.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>account</th>\n",
       "      <th>achiev</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>worri</th>\n",
       "      <th>worst</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148562</td>\n",
       "      <td>0.059306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042106</td>\n",
       "      <td>0.081248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019668</td>\n",
       "      <td>0.058811</td>\n",
       "      <td>0.033913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076317</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188925</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051538</td>\n",
       "      <td>0.038526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039842</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036591</td>\n",
       "      <td>0.023535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034008</td>\n",
       "      <td>0.084926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039351</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.088072</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.038096</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297688</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.048978</td>\n",
       "      <td>0.006668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032269</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 652 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abil       abl   absolut    accept    access  accomplish  account  \\\n",
       "0  0.148562  0.059306  0.000000  0.000000  0.000000    0.000000      0.0   \n",
       "1  0.000000  0.019668  0.058811  0.033913  0.000000    0.000000      0.0   \n",
       "2  0.000000  0.051538  0.038526  0.000000  0.000000    0.075555      0.0   \n",
       "3  0.039351  0.011782  0.088072  0.006771  0.058675    0.005757      0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.034646    0.000000      0.0   \n",
       "\n",
       "     achiev       act  action    ...         word     world     worri  \\\n",
       "0  0.045847  0.000000     0.0    ...     0.042106  0.081248  0.000000   \n",
       "1  0.000000  0.030831     0.0    ...     0.000000  0.000000  0.000000   \n",
       "2  0.039842  0.040394     0.0    ...     0.036591  0.023535  0.000000   \n",
       "3  0.000000  0.018469     0.0    ...     0.011153  0.032281  0.032597   \n",
       "4  0.032269  0.032716     0.0    ...     0.000000  0.057184  0.000000   \n",
       "\n",
       "      worst     write     wrong      yeah      york     young   younger  \n",
       "0  0.000000  0.000000  0.039135  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.076317  0.027753  0.000000  0.000000  0.188925  0.022299  0.000000  \n",
       "2  0.000000  0.000000  0.034008  0.084926  0.000000  0.000000  0.000000  \n",
       "3  0.038096  0.011083  0.000000  0.297688  0.006287  0.048978  0.006668  \n",
       "4  0.040491  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 652 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compani       22.327875\n",
       "busi          17.700765\n",
       "women         13.871414\n",
       "team          13.071614\n",
       "said          12.603874\n",
       "tri           11.874612\n",
       "got           11.405127\n",
       "great         11.224105\n",
       "manag         11.103179\n",
       "talk          11.076564\n",
       "new           11.006413\n",
       "feel          10.413546\n",
       "chang          9.978805\n",
       "love           9.949164\n",
       "custom         9.559801\n",
       "life           9.366111\n",
       "kind           9.327042\n",
       "someon         9.099354\n",
       "understand     9.033808\n",
       "organ          8.922147\n",
       "idea           8.841359\n",
       "whi            8.613173\n",
       "mani           8.605356\n",
       "world          8.419778\n",
       "experi         8.414880\n",
       "school         8.355566\n",
       "actual         8.352009\n",
       "use            8.280901\n",
       "product        8.141315\n",
       "better         8.115765\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sum().sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2490.5984630578696"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting two grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_two = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range = (2, 2),\n",
    "                             min_df = 0.2,\n",
    "                             max_df = 0.9, # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # '[a-zA-Z0-9]{3,}' num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized_two = vectorizer_two.fit_transform(interviews['data_words_stemmed_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer_two.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_words = pd.DataFrame(data_vectorized_two.toarray(),\n",
    "         columns = vectorizer_two.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make sure        27.147574\n",
       "littl bit        19.501880\n",
       "everi day        18.749676\n",
       "feel like        18.399034\n",
       "new york         15.650071\n",
       "year ago         15.023487\n",
       "make decis       14.944962\n",
       "lot peopl        14.797436\n",
       "high school      14.697088\n",
       "want know        14.289819\n",
       "realli import    13.975514\n",
       "veri import      13.948978\n",
       "ask question     13.710688\n",
       "peopl work       13.302448\n",
       "look like        12.910712\n",
       "did know         12.734489\n",
       "peopl think      12.399958\n",
       "year old         12.311241\n",
       "look peopl       12.178322\n",
       "peopl want       11.988607\n",
       "want peopl       11.900998\n",
       "realli want      11.831535\n",
       "lot time         11.407618\n",
       "work hard        11.002853\n",
       "import thing     10.609615\n",
       "peopl say        10.590614\n",
       "know know        10.465649\n",
       "tell peopl       10.399031\n",
       "peopl come       10.171517\n",
       "peopl realli     10.104671\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_words.sum().sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7789"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_words.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting three grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_three = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range = (3, 3),\n",
    "                             # min_df = 0.2,\n",
    "                             # max_df = 0.9, # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # '[a-zA-Z0-9]{3,}' num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer_three.fit_transform(interviews['data_words_stemmed_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263438"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer_three.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_words = pd.DataFrame(data_vectorized.toarray(), \n",
    "                            columns = vectorizer_three.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new york citi          0.599735\n",
       "spend lot time         0.500827\n",
       "want make sure         0.450687\n",
       "spent lot time         0.402594\n",
       "work realli hard       0.379655\n",
       "veri young age         0.356402\n",
       "make thing happen      0.336625\n",
       "ask lot question       0.336490\n",
       "work life balanc       0.322759\n",
       "make sure peopl        0.322153\n",
       "everi singl day        0.317982\n",
       "talk littl bit         0.316460\n",
       "work everi day         0.286458\n",
       "think realli import    0.273681\n",
       "want feel like         0.264513\n",
       "make sure everyon      0.251272\n",
       "women care global      0.236070\n",
       "think import thing     0.235974\n",
       "ask question like      0.226431\n",
       "think lot peopl        0.226135\n",
       "yeah yeah yeah         0.224939\n",
       "strong work ethic      0.215591\n",
       "did realli know        0.210465\n",
       "want thi job           0.209704\n",
       "outsid comfort zone    0.206853\n",
       "know know know         0.205768\n",
       "peopl realli want      0.203306\n",
       "make peopl feel        0.200421\n",
       "blah blah blah         0.199846\n",
       "harvard busi school    0.197954\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_words.sum().sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273652"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_words.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
